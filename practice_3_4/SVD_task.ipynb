{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f5407f9",
      "metadata": {
        "id": "6f5407f9"
      },
      "source": [
        "# Singular Value Decomposition (SVD)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e7c222",
      "metadata": {
        "id": "74e7c222"
      },
      "source": [
        "**Разложение по сингулярным значениям** (SVD) — это рабочая лошадка в приложениях проекции наименьших квадратов, которая \n",
        "составляют основу для многих статистических методов и методов машинного обучения. \n",
        "\n",
        "После определения SVD мы опишем, как он подключается к \n",
        "\n",
        "- **четыре фундаментальных пространства** линейной алгебры \n",
        "- недоопределенные и переопределенные **регрессии наименьших квадратов** \n",
        "- **Анализ главных компонентов** (PCA) \n",
        "\n",
        "\n",
        "Как и анализ главных компонентов (PCA), DMD можно рассматривать как процедуру сокращения данных, которая представляет существенные закономерности путем проецирования данных на ограниченный набор факторов.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d919c39",
      "metadata": {
        "id": "5d919c39"
      },
      "source": [
        "## Настройка \n",
        "\n",
        "Пусть $X$ — матрица $m\\times n$ ранга $p$. \n",
        "\n",
        "Обязательно $p\\leq\\min(m,n)$. \n",
        "\n",
        "На протяжении большей части этой лекции мы будем думать о $X$ как о матрице **данных**, в которой \n",
        "\n",
        "- в каждом столбце указано **индивидуальное** – период времени или человек, в зависимости от приложения. \n",
        "- каждая строка представляет собой **случайную величину**, описывающую атрибут периода времени или человека, в зависимости от приложения \n",
        "\n",
        "\n",
        "Нас будут интересовать две ситуации \n",
        "\n",
        "- **1** случай (short and fat) в котором $ m << n $, так что столбцов (отдельных лиц) намного больше, чем строк (атрибутов). \n",
        "- **2** случай (tall and skinny), в котором $m >> n$, так что строк (атрибутов) гораздо больше, чем столбцов (отдельных лиц). \n",
        "\n",
        "\n",
        "В обеих ситуациях мы применим **разложение по сингулярным значениям** $X$. \n",
        "\n",
        "В случае $ m < < n $, когда особей $ n $ гораздо больше, чем атрибутов $ m $, мы можем вычислить моменты выборки совместного распределения, взяв средние значения по наблюдениям функций наблюдений. \n",
        "\n",
        "В этом случае $ m < < n $ мы будем искать **шаблоны**, используя **разложение по сингулярным значениям** для выполнения **анализа главных компонентов** (PCA). \n",
        "\n",
        "В случае $ m >> n $, когда атрибутов $ m $ намного больше, чем индивидов $ n $, и когда мы находимся в настройке временного ряда, в которой $ n $ равно количеству периодов времени, охватываемых набором данных $ X $, мы поступим другим способом. \n",
        "\n",
        "Мы снова будем использовать **разложение по сингулярным значениям**, но теперь для построения **разложения по динамическим режимам** (DMD)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a008ea21",
      "metadata": {
        "id": "a008ea21"
      },
      "source": [
        "## Singular Value Decomposition\n",
        "\n",
        "\n",
        "**Разложение по сингулярным значениям** $ m \\times n $ матрицы $ X $ ранга $ p \\leq \\min(m,n) $ есть\n",
        "\n",
        "<a id='equation-eq-svd101'></a>\n",
        "$$\n",
        "X  = U \\Sigma V^\\top \\tag{5.1}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "UU^\\top  &  = I  &  \\quad U^\\top  U = I \\cr\n",
        "VV^\\top  & = I & \\quad V^\\top  V = I\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "-$U$ — $m\\times m$ ортогональная матрица **левых сингулярных векторов** матрицы $X$ \n",
        "- Столбцы $U$ являются собственными векторами $X X^\\top$ \n",
        "- $V$ — это $n\\times n$ ортогональная матрица **правых сингулярных векторов** матрицы $X$ \n",
        "- Столбцы $V$ являются собственными векторами $X^\\top X$ \n",
        "- $\\Sigma$ — $m\\times n$-матрица, в которой первые $p$ места на ее главной диагонали являются положительными числами $\\sigma_1, \\sigma_2, \\ldots, \\sigma_p$, называемыми **сингулярными значениями**; все остальные записи $\\Sigma$ равны нулю \n",
        "- Сингулярные значения $p$ являются положительными квадратными корнями из собственных значений матрицы $m\\times m$ $X X^\\top$, а также матрицы $n\\times n$ $X^\\top X$ \n",
        "- Мы принимаем соглашение, согласно которому, когда $ U $ является комплексной матрицей, $ U^\\top $ обозначает **сопряженное транспонирование** или **эрмитово транспонирование** $ U $, что означает, что \n",
        "$ U_{ij}^\\top$ — комплексно-сопряженный элемент $U_{ji}$. \n",
        "- Аналогично, когда $V$ является комплексной матрицей, $V^\\top$ обозначает **сопряженное транспонирование** или **эрмитово транспонирование** $V $.\n",
        "\n",
        "\n",
        "Матрицы $U,\\Sigma,V$ влекут за собой линейные преобразования, изменяющие форму векторов следующими способами: \n",
        "\n",
        "- умножение векторов на унитарные матрицы $U$ и $V$ **поворачивает** их, но оставляет **углы между векторами** и **длины векторов** неизменными. \n",
        "- умножение векторов на диагональную матрицу $\\Sigma$ оставляет **углы между векторами** неизменными, но **перемасштабирует** векторы.\n",
        "\n",
        "\n",
        "Таким образом, представление [(5.1)](#equation-eq-svd101) утверждает, что умножение $ n \\times 1$ вектора $y$ на $m \\times n$ матрицу $X$ \n",
        "сводится к последовательному выполнению следующих трёх умножений $y$:\n",
        "\n",
        "- **вращение** $ y $ путем вычисления $ V^\\top y $ \n",
        "- **масштабирование** $ V^\\top y $ путем умножения на $ \\Sigma $ \n",
        "- **вращение** $ \\Sigma V^\\top y $ путем умножения на $ U $\n",
        "\n",
        "\n",
        "Такая структура матрицы $m\\times n$ $X$ открывает двери для построения систем \n",
        "**кодеров** и **декодеров** данных.\n",
        "\n",
        "Таким образом, \n",
        "\n",
        "- $V^\\top y$ — кодировщик \n",
        "- $\\Sigma$ — оператор, применяемый к закодированным данным \n",
        "— $U$ — декодер, который будет применяться к выходу применения оператора $\\Sigma$ к закодированным данным\n",
        "\n",
        "\n",
        "Мы применим этот круг идей позже в этой лекции, когда будем изучать декомпозицию по динамическому режиму.\n",
        "\n",
        "\n",
        "То, что мы описали выше, называется **полной** СВД. \n",
        "\n",
        "В **полном** SVD формы $U$, $\\Sigma$ и $V$ имеют вид $\\left(m, m\\right)$, $\\left(m, n\\right)$, $\\left(n, n\\right)$ соответственно. \n",
        "\n",
        "Позже мы также опишем **экономную** или **уменьшенную** СВД. \n",
        "\n",
        "Прежде чем мы приступим к изучению **уменьшенного** СВД, скажем немного больше о свойствах **полного** СВД."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e30a6ae",
      "metadata": {
        "id": "5e30a6ae"
      },
      "source": [
        "## Четыре фундаментальных подпространства \n",
        "\n",
        "Пусть $ {\\mathcal C} $ обозначает пространство столбцов, $ {\\mathcal N} $ обозначает нулевое пространство и $ {\\mathcal R} $ обозначает пространство строк. \n",
        "\n",
        "Начнем с напоминания о четырех фундаментальных подпространствах $m\\times n$ \n",
        "матрица $X$ ранга $p$. \n",
        "\n",
        "— **Пространство столбцов** $X$, обозначаемое ${\\mathcal C}(X)$, представляет собой диапазон столбцов $X$, т. е. всех векторов $y$, которые можно записать как линейные комбинации столбцов $X$. Его размерность $p$. \n",
        "- **Нулевое пространство** $ X $, обозначаемое $ {\\mathcal N}(X) $, состоит из всех векторов $ y $, удовлетворяющих условиям \n",
        "$Ху=0$. Его размерность $n-p$. \n",
        "- **Пространство строк** $ X $, обозначаемое $ {\\mathcal R}(X) $, является пространством столбцов $ X^\\top $. Он состоит из всех \n",
        "векторы $z$, которые можно записать как линейные комбинации строк $X$. Его размерность $p$. \n",
        "- **Левое нулевое пространство** $ X $, обозначаемое $ {\\mathcal N}(X^\\top ) $, состоит из всех векторов $ z $ таких, что \n",
        "$ X^\\top z =0 $. Его размерность $m-p$.\n",
        "\n",
        "Для полной СВД матрицы $X$ матрица $U$ левых сингулярных векторов и матрица $V$ правых сингулярных векторов содержат ортогональные базисы для всех четырёх подпространств. \n",
        "\n",
        "Они образуют две пары ортогональных подпространств, что мы сейчас опишем.\n",
        "\n",
        "Let $ u_i, i = 1, \\ldots, m $ be the $ m $ column vectors of $ U $ and let\n",
        "$ v_i, i = 1, \\ldots, n $ be the $ n $ column vectors of $ V $.\n",
        "\n",
        "Давайте запишем полный SVD X как\n",
        "\n",
        "<a id='equation-eq-fullsvdpartition'></a>\n",
        "$$\n",
        "X = \\begin{bmatrix} U_L & U_R \\end{bmatrix} \\begin{bmatrix} \\Sigma_p & 0 \\cr 0 & 0 \\end{bmatrix}\n",
        "     \\begin{bmatrix} V_L & V_R \\end{bmatrix}^\\top \\tag{5.2}\n",
        "$$\n",
        "где $\\Sigma_p$ — диагональная матрица $p\\times p$ с сингулярными значениями $p$ на диагонали и\n",
        "$$\n",
        "\\begin{aligned}\n",
        "U_L & = \\begin{bmatrix}u_1 & \\cdots  & u_p \\end{bmatrix},  \\quad U_R  = \\begin{bmatrix}u_{p+1} & \\cdots u_m \\end{bmatrix}  \\cr\n",
        "V_L & = \\begin{bmatrix}v_1 & \\cdots  & v_p \\end{bmatrix} , \\quad U_R  = \\begin{bmatrix}v_{p+1} & \\cdots u_n \\end{bmatrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Представление [(5.2)](#equation-eq-fullsvdpartition) подразумевает, что\n",
        "\n",
        "$$\n",
        "X \\begin{bmatrix} V_L & V_R \\end{bmatrix} = \\begin{bmatrix} U_L & U_R \\end{bmatrix} \\begin{bmatrix} \\Sigma_p & 0 \\cr 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "или\n",
        "\n",
        "\n",
        "<a id='equation-eq-xfour1a'></a>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "X V_L & = U_L \\Sigma_p \\cr\n",
        "X V_R & = 0\n",
        "\\end{aligned} \\tag{5.3}\n",
        "$$\n",
        "\n",
        "или\n",
        "\n",
        "\n",
        "<a id='equation-eq-orthoortho1'></a>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "X v_i & = \\sigma_i u_i , \\quad i = 1, \\ldots, p \\cr\n",
        "X v_i & = 0 ,  \\quad i = p+1, \\ldots, n\n",
        "\\end{aligned} \\tag{5.4}\n",
        "$$\n",
        "\n",
        "Equations [(5.4)](#equation-eq-orthoortho1) tell how the transformation $ X $ maps a pair of orthonormal  vectors $ v_i, v_j $ for $ i $ and $ j $ both less than or equal to the rank $ p $ of $ X $ into a pair of orthonormal vectors $ u_i, u_j $.\n",
        "\n",
        "Equations [(5.3)](#equation-eq-xfour1a) assert that\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "{\\mathcal C}(X) & = {\\mathcal C}(U_L) \\cr\n",
        "{\\mathcal N}(X) & = {\\mathcal C} (V_R)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Taking transposes on both sides of representation [(5.2)](#equation-eq-fullsvdpartition) implies\n",
        "\n",
        "$$\n",
        "X^\\top  \\begin{bmatrix} U_L & U_R \\end{bmatrix} = \\begin{bmatrix} V_L & V_R \\end{bmatrix} \\begin{bmatrix} \\Sigma_p & 0 \\cr 0 & 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "или\n",
        "\n",
        "\n",
        "<a id='equation-eq-xfour1b'></a>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "X^\\top  U_L & = V_L \\Sigma_p \\cr\n",
        "X^\\top  U_R & = 0\n",
        "\\end{aligned} \\tag{5.5}\n",
        "$$\n",
        "\n",
        "или\n",
        "\n",
        "\n",
        "<a id='equation-eq-orthoortho2'></a>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "X^\\top  u_i & = \\sigma_i v_i, \\quad i=1, \\ldots, p \\cr\n",
        "X^\\top  u_i & = 0 \\quad i= p+1, \\ldots, m\n",
        "\\end{aligned} \\tag{5.6}\n",
        "$$\n",
        "\n",
        "Обратите внимание, как уравнения [(5.6)](#equation-eq-orthoortho2) утверждают, что преобразование $X^\\top$ отображает пару различных ортонормированных векторов $u_i, u_j$ для $i$ и $j$, которые меньше или равны рангу $p$ $X$, в пару различных ортонормированных векторов $v_i, v_j$.\n",
        "\n",
        "Equations [(5.5)](#equation-eq-xfour1b) assert that\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "{\\mathcal R}(X) & \\equiv  {\\mathcal C}(X^\\top ) = {\\mathcal C} (V_L) \\cr\n",
        "{\\mathcal N}(X^\\top ) & = {\\mathcal C}(U_R)\n",
        "\\end{aligned}\n",
        "$$\n",
        "Таким образом, в совокупности системы уравнений [(5.3)](#equation-eq-xfour1a) и [(5.5)](#equation-eq-xfour1b) \n",
        "опишите четыре фундаментальных подпространства $X$ следующими способами:\n",
        "\n",
        "\n",
        "<a id='equation-eq-fourspacesvd'></a>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "{\\mathcal C}(X) & = {\\mathcal C}(U_L) \\cr\n",
        "{\\mathcal N}(X^\\top ) & = {\\mathcal C}(U_R) \\cr\n",
        "{\\mathcal R}(X) & \\equiv  {\\mathcal C}(X^\\top ) = {\\mathcal C} (V_L) \\cr\n",
        "{\\mathcal N}(X) & = {\\mathcal C} (V_R) \\cr\n",
        "\n",
        "\\end{aligned} \\tag{5.7}\n",
        "$$\n",
        "\n",
        "Поскольку $U$ и $V$ являются ортонормированными матрицами, набор [(5.7)](#equation-eq-fourspacesvd) утверждает, что \n",
        "\n",
        "— $U_L$ — ортонормированный базис пространства столбцов $X$ \n",
        "- $U_R$ — ортонормированный базис нуль-пространства $X^\\top$ \n",
        "- $V_L$ — ортонормированный базис для пространства строк $X$ \n",
        "— $V_R$ — ортонормированный базис нулевого пространства $X$\n",
        "\n",
        "\n",
        "Мы проверили четыре утверждения в [(5.7)](#equation-eq-fourspacesvd), просто выполнив умножения, требуемые в правой части [(5.2)](#equation-eq-fullsvdpartition), и прочитав их. \n",
        "\n",
        "Утверждения в [(5.7)](#equation-eq-fourspacesvd) и тот факт, что $U$ и $V$ являются унитарными (т.е. ортонормированными) матрицами, подразумевают \n",
        "что \n",
        "\n",
        "- пространство столбцов $X$ ортогонально пустому пространству $X^\\top$ \n",
        "- нулевое пространство $X$ ортогонально пространству строк $X$ \n",
        "\n",
        "\n",
        "Иногда эти свойства описываются следующими двумя парами ортогональных дополнительных подпространств:\n",
        "\n",
        "- $ {\\mathcal C}(X) $ is the orthogonal complement of $ {\\mathcal N}(X^\\top ) $  \n",
        "- $ {\\mathcal R}(X) $ is the orthogonal complement  $ {\\mathcal N}(X) $  \n",
        "\n",
        "\n",
        "Давайте сделаем пример."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c1d66623",
      "metadata": {
        "hide-output": false,
        "id": "c1d66623"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as LA\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d71ce9",
      "metadata": {
        "id": "14d71ce9"
      },
      "source": [
        "Having imported these modules, let’s do the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aedb0caa",
      "metadata": {
        "hide-output": false,
        "id": "aedb0caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank of matrix:\n",
            " 2\n",
            "S: \n",
            " [2.69e+01 1.86e+00 7.83e-16 3.27e-16 4.69e-17]\n",
            "U:\n",
            " [[-0.27 -0.73 -0.47  0.06 -0.42]\n",
            " [-0.35 -0.42  0.1  -0.18  0.81]\n",
            " [-0.43 -0.11  0.75 -0.27 -0.4 ]\n",
            " [-0.51  0.19  0.06  0.83  0.05]\n",
            " [-0.59  0.5  -0.45 -0.45 -0.04]]\n",
            "Column space:\n",
            " [[-0.27 -0.35]\n",
            " [ 0.73  0.42]\n",
            " [ 0.02  0.06]\n",
            " [ 0.52 -0.83]\n",
            " [-0.37 -0.08]]\n",
            "Left null space:\n",
            " [[-0.47  0.06 -0.42]\n",
            " [ 0.1  -0.18  0.81]\n",
            " [ 0.75 -0.27 -0.4 ]\n",
            " [ 0.06  0.83  0.05]\n",
            " [-0.45 -0.45 -0.04]]\n",
            "V.T:\n",
            " [[-0.27  0.73  0.02  0.52 -0.37]\n",
            " [-0.35  0.42  0.06 -0.83 -0.08]\n",
            " [-0.43  0.11  0.29  0.18  0.82]\n",
            " [-0.51 -0.19 -0.83  0.06  0.04]\n",
            " [-0.59 -0.5   0.46  0.07 -0.42]]\n",
            "Row space:\n",
            " [[-0.27 -0.35 -0.43 -0.51 -0.59]\n",
            " [-0.73 -0.42 -0.11  0.19  0.5 ]]\n",
            "Right null space:\n",
            " [[-0.43  0.11  0.29  0.18  0.82]\n",
            " [-0.51 -0.19 -0.83  0.06  0.04]\n",
            " [-0.59 -0.5   0.46  0.07 -0.42]]\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Define the matrix\n",
        "A = np.array([[1, 2, 3, 4, 5],\n",
        "              [2, 3, 4, 5, 6],\n",
        "              [3, 4, 5, 6, 7],\n",
        "              [4, 5, 6, 7, 8],\n",
        "              [5, 6, 7, 8, 9]])\n",
        "\n",
        "# Вычислите SVD матрицы c помощью np.linalg.svd\n",
        " ваш_код\n",
        "\n",
        "# Вычислить ранг матрицы linalg.matrix_rank\n",
        " ваш_код\n",
        "\n",
        "# Вывести все на печать\n",
        " ваш_код\n",
        "\n",
        "# Вфчислите 4 фундаментальных пространства\n",
        "row_space =  \n",
        "col_space =  \n",
        "null_space =  \n",
        "left_null_space = \n",
        "\n",
        "# вывести все на печать\n",
        "print(\"U:\\n\",  )\n",
        "print(\"Column space:\\n\", )\n",
        "print(\"Left null space:\\n\", )\n",
        "print(\"V.T:\\n\",  )\n",
        "print(\"Row space:\\n\",  )\n",
        "print(\"Right null space:\\n\",  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11bfc3f",
      "metadata": {
        "id": "c11bfc3f"
      },
      "source": [
        "## Теорема Эккарта-Янга\n",
        "\n",
        "Предположим, что мы хотим построить наилучшее приближение ранга $r$ $m\\times n$-матрицы $X$. \n",
        "\n",
        "Под наилучшей мы понимаем матрицу $X_r$ ранга $r<p$, которая среди всех матриц ранга $r$ минимизирует\n",
        "\n",
        "$$\n",
        "|| X - X_r ||\n",
        "$$\n",
        "\n",
        "где $ || \\cdot || $ обозначает норму матрицы $X$ и где $X_r$ принадлежит пространству всех матриц ранга $r$ \n",
        "размерности $m\\times n$.\n",
        "\n",
        "Три популярные **матричные нормы** $ m \\times n$ матрицы $ X $ можно выразить через сингулярные значения $ X $ \n",
        "\n",
        "- **спектральная** или $l^2$ норма $ || X ||_2 = \\max_{||y|| \\neq 0} \\frac{||X y ||}{||y||} = \\sigma_1 $ \n",
        "- норма **Фробениуса** $ ||X ||_F = \\sqrt{\\sigma_1^2 + \\cdots + \\sigma_p^2} $ \n",
        "- **ядерная** норма $ || X ||_N = \\sigma_1 + \\cdots + \\sigma_p $\n",
        "\n",
        "Теорема Эккарта-Янга утверждает, что для каждой из этих трех норм матрица $r$ одного и того же ранга является наилучшей и что она равна\n",
        "\n",
        "<a id='equation-eq-ekart'></a>\n",
        "$$\n",
        "\\hat X_r = \\sigma_1 U_1 V_1^\\top  + \\sigma_2 U_2 V_2^\\top  + \\cdots + \\sigma_r U_r V_r^\\top \\tag{5.8}\n",
        "$$\n",
        "\n",
        "Это очень мощная теорема, которая гласит, что мы можем взять нашу $m\\times n$матрицу $X$, которая не в полном ранге, и лучше всего аппроксимировать ее матрицей $p\\times p$ полного ранга через SVD. \n",
        "\n",
        "Более того, если некоторые из этих $p$ сингулярных значений несут больше информации, чем другие, и если мы хотим иметь наибольшее количество информации с наименьшим количеством данных, мы можем взять $r$ ведущие сингулярные значения, упорядоченные по величине. \n",
        "\n",
        "Мы расскажем об этом подробнее позже, когда представим анализ главных компонентов. \n",
        "\n",
        "Вы можете прочитать о теореме Эккарта-Янга и некоторых ее применениях [здесь] (https://en.wikipedia.org/wiki/Low-rank_approximation). \n",
        "\n",
        "Мы будем использовать эту теорему, когда будем обсуждать анализ главных компонентов (PCA), а также динамическую модовую декомпозицию (DMD)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94971858",
      "metadata": {
        "id": "94971858"
      },
      "source": [
        "## Полные и уменьшенные СВД\n",
        "\n",
        "До сих пор мы описывали свойства **полного** SVD, в котором формы $U$, $\\Sigma$ и $V$ имеют вид $\\left(m, m\\right)$, $\\left(m, n\\right)$, $\\left(n, n\\right)$ соответственно. \n",
        "\n",
        "Существует альтернативное бухгалтерское соглашение, называемое **экономичным** или **уменьшенным** SVD, в котором формы $U, \\Sigma$ и $V$ отличаются от того, что они имеют в полном SVD. \n",
        "\n",
        "Таким образом, обратите внимание: поскольку мы предполагаем, что $X$ имеет ранг $p$, существуют только $p$ ненулевые сингулярные значения, где $p=\\textrm{rank}(X)\\leq\\min\\left(m, n\\right)$. \n",
        "\n",
        "**Приведенный** SVD использует этот факт для выражения $U$, $\\Sigma$ и $V$ в виде матриц с формами $\\left(m, p\\right)$, $\\left(p, p\\right)$, $\\left( n, p\\right)$. \n",
        "\n",
        "Про уменьшенную и полную СВД можно прочитать здесь.\n",
        "[https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n",
        "\n",
        "Для полной SVD,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "UU^\\top  &  = I  &  \\quad U^\\top  U = I \\cr\n",
        "VV^\\top  & = I & \\quad V^\\top  V = I\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Bоднако не все эти свойства справедливы для **уменьшенной** СВД. \n",
        "\n",
        "Какие свойства сохраняются, зависит от того, находимся ли мы в случае **высоких-худых** или в случае **низких-толстых**. \n",
        "\n",
        "- В **высоком-худом** случае, когда $m > > n$, для **уменьшенной** СВД\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "UU^\\top  &  \\neq I  &  \\quad U^\\top  U = I \\cr\n",
        "VV^\\top  & = I & \\quad V^\\top  V = I\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- **Короткий** случай, когда $ m < < n $, для **уменьшенного** SVD\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "UU^\\top  &  = I  &  \\quad U^\\top  U = I \\cr\n",
        "VV^\\top  & = I & \\quad V^\\top  V \\neq I\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Когда мы ниже будем изучать динамическую модовую декомпозицию, мы захотим запомнить эти свойства, когда будем использовать сокращенный SVD для вычисления некоторых представлений DMD. \n",
        "\n",
        "Давайте выполним упражнение, чтобы сравнить **полные** и **уменьшенные** SVD.\n",
        "Чтобы просмотреть,\n",
        "\n",
        "- in a **full** SVD  \n",
        "  - $ U $ is $ m \\times m $  \n",
        "  - $ \\Sigma $ is $ m \\times n $  \n",
        "  - $ V $ is $ n \\times n $  \n",
        "- in a **reduced** SVD  \n",
        "  - $ U $ is $ m \\times p $  \n",
        "  - $ \\Sigma $ is $ p\\times p $  \n",
        "  - $ V $ is $ n \\times p $  \n",
        "\n",
        "Сначала рассмотрим случай, когда $m = 5 > n = 2$. \n",
        "\n",
        "(Это небольшой пример случая **высокий-худой**, который будет интересовать нас, когда мы будем изучать **Динамическое разложение по модам** ниже.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c46039e",
      "metadata": {
        "hide-output": false,
        "id": "6c46039e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U, S, V =\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[-0.58,  0.55, -0.5 , -0.23,  0.25],\n",
              "        [-0.44, -0.1 , -0.09,  0.11, -0.88],\n",
              "        [-0.53,  0.08,  0.83, -0.09,  0.16],\n",
              "        [-0.13,  0.16, -0.03,  0.96,  0.17],\n",
              "        [-0.42, -0.81, -0.25,  0.01,  0.33]]),\n",
              " array([1.61, 0.77]),\n",
              " array([[-0.62, -0.79],\n",
              "        [-0.79,  0.62]]))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "X = np.random.rand(5,2)\n",
        "U, S, V = np.linalg.svd( ваш_код )  # full SVD\n",
        "Uhat, Shat, Vhat = np.linalg.svd(  ваш_код) # economy SVD\n",
        "print('U, S, V =')\n",
        "U, S, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "18892ee2",
      "metadata": {
        "hide-output": false,
        "id": "18892ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uhat, Shat, Vhat = \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[-0.58,  0.55],\n",
              "        [-0.44, -0.1 ],\n",
              "        [-0.53,  0.08],\n",
              "        [-0.13,  0.16],\n",
              "        [-0.42, -0.81]]),\n",
              " array([1.61, 0.77]),\n",
              " array([[-0.62, -0.79],\n",
              "        [-0.79,  0.62]]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('Uhat, Shat, Vhat = ')\n",
        "Uhat, Shat, Vhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0a1e746a",
      "metadata": {
        "hide-output": false,
        "id": "0a1e746a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rank of X = 2\n"
          ]
        }
      ],
      "source": [
        "rr = np.linalg.matrix_rank(X)\n",
        "print(f'rank of X = {rr}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f20962",
      "metadata": {
        "id": "d4f20962"
      },
      "source": [
        "**Свойства:** \n",
        "\n",
        "- Где $U$ строится через полный СВД, $U^\\top U = I_{m\\times m} $ и $ U U^\\top = I_{m \\times m} $ \n",
        "- Там, где $\\hat U$ строится посредством редуцированного СВД, хотя $\\hat U^\\top\\hat U = I_{p\\times p}$, бывает, что $\\hat U \\hat U^\\top \\neq I_{m \\times m} $ \n",
        "\n",
        "\n",
        "Мы проиллюстрируем эти свойства для нашего примера с помощью следующих ячеек кода."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de21b506",
      "metadata": {
        "hide-output": false,
        "id": "de21b506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UUT, UTU = \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[ 1.00e+00,  2.15e-16, -3.87e-17, -4.10e-17,  3.08e-17],\n",
              "        [ 2.15e-16,  1.00e+00,  1.19e-16,  4.11e-17,  4.04e-17],\n",
              "        [-3.87e-17,  1.19e-16,  1.00e+00,  2.87e-18,  2.99e-17],\n",
              "        [-4.10e-17,  4.11e-17,  2.87e-18,  1.00e+00, -1.35e-17],\n",
              "        [ 3.08e-17,  4.04e-17,  2.99e-17, -1.35e-17,  1.00e+00]]),\n",
              " array([[ 1.00e+00,  1.15e-16, -2.69e-17, -1.48e-17,  1.23e-16],\n",
              "        [ 1.15e-16,  1.00e+00,  6.63e-17,  5.08e-17, -1.63e-16],\n",
              "        [-2.69e-17,  6.63e-17,  1.00e+00,  8.40e-18,  4.87e-17],\n",
              "        [-1.48e-17,  5.08e-17,  8.40e-18,  1.00e+00,  1.08e-17],\n",
              "        [ 1.23e-16, -1.63e-16,  4.87e-17,  1.08e-17,  1.00e+00]]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "UTU =  ваш_код\n",
        "UUT =  ваш_код\n",
        "print('UUT, UTU = ')\n",
        "UUT, UTU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169182d9",
      "metadata": {
        "hide-output": false,
        "id": "169182d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UhatUhatT, UhatTUhat= \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[ 0.64,  0.2 ,  0.35,  0.17, -0.2 ],\n",
              "        [ 0.2 ,  0.21,  0.23,  0.04,  0.27],\n",
              "        [ 0.35,  0.23,  0.29,  0.08,  0.15],\n",
              "        [ 0.17,  0.04,  0.08,  0.04, -0.08],\n",
              "        [-0.2 ,  0.27,  0.15, -0.08,  0.83]]),\n",
              " array([[1.00e+00, 1.15e-16],\n",
              "        [1.15e-16, 1.00e+00]]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "UhatUhatT =  ваш_код\n",
        "UhatTUhat =  ваш_код\n",
        "print('UhatUhatT, UhatTUhat= ')\n",
        "UhatUhatT, UhatTUhat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff6f106b",
      "metadata": {
        "id": "ff6f106b"
      },
      "source": [
        "**Примечания:** \n",
        "\n",
        "Ячейки выше иллюстрируют применение опций `full_matrices=True` и `full_matrices=False`. \n",
        "Использование `full_matrices=False` возвращает уменьшенное разложение по сингулярным значениям. \n",
        "\n",
        "И **полный**, и **уменьшенный** SVD точно разлагают $ m \\times n$ матрицу $ X $ \n",
        "\n",
        "Когда мы будем изучать динамическое разложение по модам ниже, нам будет важно помнить предыдущие свойства полных и уменьшенных SVD в таких случаях с высоким ростом. \n",
        "\n",
        "Теперь обратимся к короткому делу. \n",
        "\n",
        "Чтобы проиллюстрировать этот случай, мы установим $ m = 2 < 5 = n $ и вычислим как полные, так и сокращенные SVD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0248dc65",
      "metadata": {
        "hide-output": false,
        "id": "0248dc65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U, S, V = \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[-0.88, -0.48],\n",
              "        [-0.48,  0.88]]),\n",
              " array([1.62, 0.48]),\n",
              " array([[-0.64, -0.34, -0.29, -0.59, -0.21],\n",
              "        [-0.35, -0.32,  0.11,  0.2 ,  0.85],\n",
              "        [-0.29,  0.13,  0.92, -0.15, -0.15],\n",
              "        [-0.6 ,  0.23, -0.15,  0.69, -0.3 ],\n",
              "        [-0.18,  0.84, -0.17, -0.33,  0.34]]))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "X = np.random.rand(2,5)\n",
        "U, S, V = np.linalg.svd( ваш_код)  # full SVD\n",
        "Uhat, Shat, Vhat = np.linalg.svd(ваш_код) # economy SVD\n",
        "print('U, S, V = ')\n",
        "U, S, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f8b16f82",
      "metadata": {
        "hide-output": false,
        "id": "f8b16f82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uhat, Shat, Vhat = \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([[-0.88, -0.48],\n",
              "        [-0.48,  0.88]]),\n",
              " array([1.62, 0.48]),\n",
              " array([[-0.64, -0.34, -0.29, -0.59, -0.21],\n",
              "        [-0.35, -0.32,  0.11,  0.2 ,  0.85]]))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('Uhat, Shat, Vhat = ')\n",
        "Uhat, Shat, Vhat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f21d7ef",
      "metadata": {
        "id": "2f21d7ef"
      },
      "source": [
        "Let’s verify that our reduced SVD accurately represents $ X $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bf918c8b",
      "metadata": {
        "hide-output": false,
        "id": "bf918c8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SShat=np.diag(Shat)\n",
        "np.allclose(X, Uhat@SShat@Vhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a24266",
      "metadata": {
        "id": "58a24266"
      },
      "source": [
        "## Полярное разложение \n",
        "\n",
        "**Приведенное** разложение по сингулярным значениям (SVD) $ X $ связано с **полярным разложением** $ X $$\n",
        "\n",
        "$$\n",
        "X  = SQ\n",
        "$$\n",
        "\n",
        "где\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        " S & = U\\Sigma U^\\top  \\cr\n",
        "Q & = U V^\\top\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Здесь\n",
        "\n",
        "- $ S $ is  an $ m \\times m $  **symmetric** matrix  \n",
        "- $ Q $ is an $ m \\times n $  **orthogonal** matrix  \n",
        "\n",
        "\n",
        "и в нашей reduced SVD\n",
        "\n",
        "- $ U $ is an $ m \\times p $ orthonormal matrix  \n",
        "- $ \\Sigma $ is a $ p \\times p $ diagonal matrix  \n",
        "- $ V $ is an $ n \\times p $ orthonormal  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f8ae49",
      "metadata": {
        "id": "29f8ae49"
      },
      "source": [
        "## Приложение: анализ главных компонентов (PCA) \n",
        "\n",
        "Начнем со случая, когда $n>>m$, так что особей $n$ у нас гораздо больше, чем атрибутов $m$. \n",
        "\n",
        "Матрица $ X $ является **короткой и толстой** в случае $ n >> m $, в отличие от случая **высокой и тощей** с $ m >> n $, который будет обсуждаться позже. \n",
        "\n",
        "Мы рассматриваем $X$ как матрицу $m\\times n$ **данных**: \n",
        "\n",
        "$$ \n",
        "X = \\begin{bmatrix} X_1 \\mid X_2 \\mid \\cdots \\mid X_n\\end{bmatrix} \n",
        "$$ \n",
        "\n",
        "где для $j = 1, \\ldots, n$ вектор-столбец $X_j = \\begin{bmatrix}x_{1j}\\\\x_{2j}\\\\\\vdots\\\\x_{mj}\\end{bmatrix}$ является вектором наблюдений над переменными $ \\begin{bmatrix}X_1\\\\X_2\\\\\\vdots\\\\X_m\\end{bmatrix}$. \n",
        "\n",
        "В настройке **временных рядов** мы могли бы рассматривать столбцы $j$ как индексирующие различные **моменты**, в которые наблюдаются случайные переменные, а строки индексируют разные случайные величины. \n",
        "\n",
        "В настройке **перекрестного** мы могли бы рассматривать столбцы $ j $ как индексирующие различные **индивиды**, для которых наблюдаются случайные величины, в то время как строки индексируют различные **атрибуты**. \n",
        "\n",
        "Как мы видели ранее, SVD — это способ разложить матрицу на полезные компоненты, точно так же, как полярное разложение, собственное разложение и многие другие. \n",
        "\n",
        "PCA, с другой стороны, представляет собой метод, основанный на SVD для анализа данных. Цель состоит в том, чтобы применить определенные шаги, чтобы помочь лучше визуализировать закономерности в данных, используя статистические инструменты для выявления наиболее важных закономерностей в данных. \n",
        "\n",
        "**Шаг 1. Стандартизируйте данные:** \n",
        "\n",
        "Поскольку наша матрица данных может содержать переменные разных единиц и масштабов, сначала нам необходимо стандартизировать данные. \n",
        "\n",
        "Сначала вычисляя среднее значение каждой строки $X$. \n",
        "\n",
        "$$ \n",
        "\\bar{X_i}= \\frac{1}{n} \\sum_{j = 1}^{n} x_{ij} \n",
        "$$ \n",
        "\n",
        "Затем мы создаем среднюю матрицу из этих средств: \n",
        "\n",
        "$$ \n",
        "\\bar{X} = \\begin{bmatrix} \\bar{X_1} \\\\ \\bar{X_2} \\\\ \\ldots \\\\ \\bar{X_m}\\end{bmatrix}\\begin{bmatrix}1 \\mid 1 \\mid \\cdots \\mid 1 \\end{bmatrix} \n",
        "$$ \n",
        "\n",
        "И вычтите из исходной матрицы, чтобы создать среднецентрированную матрицу: \n",
        "\n",
        "$$ \n",
        "B = X - \\bar{X} \n",
        "$$ \n",
        "\n",
        "**Шаг 2. Вычислите ковариационную матрицу:** \n",
        "\n",
        "Затем, поскольку мы хотим извлечь связи между переменными, а не только их величину, другими словами, мы хотим знать, как они могут объяснить друг друга, мы вычисляем ковариационную матрицу $B$. \n",
        "\n",
        "$$ \n",
        "C = \\frac{1}{n} BB^{\\top} \n",
        "$$ \n",
        "\n",
        "**Шаг 3. Разложите ковариационную матрицу и расположите сингулярные значения:** \n",
        "\n",
        "Поскольку матрица $C$ положительно определена, мы можем разложить ее по собственным значениям, найти ее собственные значения и переставить матрицы собственных значений и собственных векторов в порядке убывания. \n",
        "\n",
        "Собственное разложение $C$ можно найти, разложив вместо него $B$. Так как $B$ не является квадратной матрицей, то получаем SVD $B$: \n",
        "\n",
        "$$ \n",
        "\\begin{align} \n",
        "B B^\\top &= U \\Sigma V^\\top (U \\Sigma V^{\\top})^{\\top}\\\\ \n",
        "&= U \\Sigma V^\\top V \\Sigma^\\top U^\\top\\\\ \n",
        "&= U \\Sigma \\Sigma^\\top U^\\top \n",
        "\\end{align} \n",
        "$$ \n",
        "\n",
        "$$ \n",
        "C = \\frac{1}{n} U \\Sigma \\Sigma^\\top U^\\top \n",
        "$$ \n",
        "\n",
        "Затем мы можем переставить столбцы в матрицах $U$ и $\\Sigma$ так, чтобы сингулярные значения располагались в порядке убывания. \n",
        "\n",
        "**Шаг 4. Выберите единичные значения и (необязательно) обрежьте остальные:** \n",
        "\n",
        "Теперь мы можем решить, сколько сингулярных значений выбрать, исходя из того, какую дисперсию вы хотите сохранить. (например, сохранение 95% общей дисперсии). \n",
        "\n",
        "Мы можем получить процент, рассчитав дисперсию, содержащуюся в ведущих факторах $r$, разделенную на общую дисперсию: \n",
        "\n",
        "$$ \n",
        "\\frac{\\sum_{i = 1}^{r} \\sigma^2_{i}}{\\sum_{i = 1}^{p} \\sigma^2_{i}}\n",
        "$$ \n",
        "\n",
        "**Шаг 5. Создайте матрицу оценок:** \n",
        "\n",
        "$$\n",
        "\\begin{align} \n",
        "Т&= БВ \\cr \n",
        "&= U\\Sigma V^\\top V \\cr \n",
        "&= U\\Sigma \n",
        "\\end{align} \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8edb8e4e",
      "metadata": {
        "id": "8edb8e4e"
      },
      "source": [
        "## Отношения PCA и SVD\n",
        "Чтобы связать SVD с PCA набора данных $X$, сначала постройте SVD матрицы данных $X$: \n",
        "\n",
        "Предположим, что выборочные средние всех переменных равны нулю, поэтому нам не нужно стандартизировать нашу матрицу.\n",
        "\n",
        "\n",
        "<a id='equation-eq-pca1'></a>\n",
        "$$\n",
        "X = U \\Sigma V^\\top  = \\sigma_1 U_1 V_1^\\top  + \\sigma_2 U_2 V_2^\\top  + \\cdots + \\sigma_p U_p V_p^\\top \\tag{5.9}\n",
        "$$\n",
        "\n",
        "где\n",
        "\n",
        "$$\n",
        "U=\\begin{bmatrix}U_1|U_2|\\ldots|U_m\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V^\\top  = \\begin{bmatrix}V_1^\\top \\\\V_2^\\top \\\\\\ldots\\\\V_n^\\top \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "В уравнении [(5.9)](#equation-eq-pca1) каждая из $m\\times n$ матриц $U_{j}V_{j}^\\top$ очевидно \n",
        "ранга $1$. \n",
        "\n",
        "Таким образом, мы имеем\n",
        "\n",
        "\n",
        "<a id='equation-eq-pca2'></a>\n",
        "$$\n",
        "X = \\sigma_1 \\begin{bmatrix}U_{11}V_{1}^\\top \\\\U_{21}V_{1}^\\top \\\\\\cdots\\\\U_{m1}V_{1}^\\top \\\\\\end{bmatrix} + \\sigma_2\\begin{bmatrix}U_{12}V_{2}^\\top \\\\U_{22}V_{2}^\\top \\\\\\cdots\\\\U_{m2}V_{2}^\\top \\\\\\end{bmatrix}+\\ldots + \\sigma_p\\begin{bmatrix}U_{1p}V_{p}^\\top \\\\U_{2p}V_{p}^\\top \\\\\\cdots\\\\U_{mp}V_{p}^\\top \\\\\\end{bmatrix} \\tag{5.10}\n",
        "$$\n",
        "\n",
        "Вот как мы будем интерпретировать объекты в матричном уравнении [(5.10)](#equation-eq-pca2) в \n",
        "контекст временного ряда:\n",
        "\n",
        "- $ \\textrm{for each} \\   k=1, \\ldots, n $, the object $ \\lbrace V_{kj} \\rbrace_{j=1}^n $ is a time series   for the $ k $th **principal component**  \n",
        "- $ U_j = \\begin{bmatrix}U_{1k}\\\\U_{2k}\\\\\\ldots\\\\U_{mk}\\end{bmatrix} \\  k=1, \\ldots, m $\n",
        "— вектор **нагрузок** переменных $X_i$ на $k$-ю главную компоненту, $i=1,\\ldots,m$ \n",
        "- $\\sigma_k$ для каждого $k=1,\\ldots,p$ — это сила $k$го **главного компонента**, где сила означает вклад в общую ковариацию $X$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c594f1b7",
      "metadata": {
        "id": "c594f1b7"
      },
      "source": [
        "## PCA с собственными значениями и векторами \n",
        "\n",
        "Теперь мы используем собственное разложение выборочной ковариационной матрицы для выполнения PCA. \n",
        "\n",
        "Пусть $X_{m\\times n}$ — наша $m\\times n$ матрица данных. \n",
        "\n",
        "Предположим, что выборочные средние всех переменных равны нулю. \n",
        "\n",
        "Мы можем гарантировать это, **предварительно обрабатывая** данные путем вычитания выборочных средних значений. \n",
        "\n",
        "Определите выборочную ковариационную матрицу $\\Omega$ как \n",
        "\n",
        "$$ \n",
        "\\Omega = XX^\\top \n",
        "$$ \n",
        "\n",
        "Затем используйте собственное разложение для представления $\\Omega$ следующим образом: \n",
        "\n",
        "$$ \n",
        "\\Omega =P\\Lambda P^\\top \n",
        "$$ \n",
        "\n",
        "Здесь \n",
        "\n",
        "- $P$ — $m×m$ матрица собственных векторов $\\Omega$ \n",
        "- $\\Lambda$ — диагональная матрица собственных значений $\\Omega$ \n",
        "\n",
        "\n",
        "Тогда мы можем представить $ X $ как \n",
        "\n",
        "$$ \n",
        "X=P\\epsilon \n",
        "$$ \n",
        "\n",
        "где \n",
        "\n",
        "$$ \n",
        "\\epsilon = P^{-1} X \n",
        "$$ \n",
        "\n",
        "и \n",
        "\n",
        "$$ \n",
        "\\epsilon\\epsilon^\\top =\\Lambda . \n",
        "$$ \n",
        "\n",
        "Мы можем это проверить \n",
        "\n",
        "\n",
        "<a id='equation-eq-xxo'></a> \n",
        "$$ \n",
        "XX^\\top =P\\Lambda P^\\top . \\tag{5.11} \n",
        "$$ \n",
        "\n",
        "Отсюда следует, что мы можем представить матрицу данных $X$ как \n",
        "\n",
        "$$ \n",
        "\\begin{equation*} \n",
        "X=\\begin{bmatrix}X_1|X_2|\\ldots|X_m\\end{bmatrix} =\\begin{bmatrix}P_1|P_2|\\ldots|P_m\\end{bmatrix} \n",
        "\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\ldots\\\\\\epsilon_m\\end{bmatrix} \n",
        "= P_1\\epsilon_1+P_2\\epsilon_2+\\ldots+P_m\\epsilon_m \n",
        "\\end{equation*} \n",
        "$$ \n",
        "\n",
        "Чтобы согласовать предыдущее представление с PCA, полученным нами ранее с помощью SVD, сначала отметим, что $\\epsilon_j^2=\\lambda_j\\equiv\\sigma^2_j$. \n",
        "\n",
        "Теперь определите $\\tilde{\\epsilon_j} = \\frac{\\epsilon_j}{\\sqrt{\\lambda_j}} $, \n",
        "откуда следует, что $ \\tilde{\\epsilon}_j\\tilde{\\epsilon}_j^\\top =1 $. \n",
        "\n",
        "Поэтому \n",
        "\n",
        "$$ \n",
        "\\begin{align} \n",
        "X&=\\sqrt{\\lambda_1}P_1\\tilde{\\epsilon_1}+\\sqrt{\\lambda_2}P_2\\tilde{\\epsilon_2}+\\ldots+\\sqrt{\\lambda_m}P_m\\tilde{\\epsilon_m}\\\\ \n",
        "&=\\sigma_1P_1\\tilde{\\epsilon_2}+\\sigma_2P_2\\tilde{\\epsilon_2}+\\ldots+\\sigma_mP_m\\tilde{\\epsilon_m} , \n",
        "\\end{align} \n",
        "$$ \n",
        "\n",
        "что согласуется с \n",
        "\n",
        "$$ \n",
        "X=\\sigma_1U_1{V_1}^{T}+\\sigma_2 U_2{V_2}^{T}+\\ldots+\\sigma_{r} U_{r}{V_{r}}^{T} \n",
        "$$ \n",
        "\n",
        "при условии, что мы установим \n",
        "\n",
        "- $U_j=P_j$ (вектор нагрузок переменных на главную компоненту $j$) \n",
        "- $ {V_k}^{T}=\\tilde{\\epsilon_k} $ ($ k $-я главная компонента) \n",
        "\n",
        "\n",
        "Поскольку существуют альтернативные алгоритмы вычисления $P$ и $U$ для заданной матрицы данных $X$, в зависимости от используемых алгоритмов у нас могут быть различия в знаках или разные порядки собственных векторов. \n",
        "\n",
        "Мы можем разрешить подобные неоднозначности относительно $U$ и $P$ с помощью \n",
        "\n",
        "1. сортировка собственных значений и сингулярных значений в порядке убывания. \n",
        "1. наложение положительных диагоналей на $P$ и $U$ и корректировка знаков в $V^\\top$ соответственно"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d6cf1d",
      "metadata": {
        "id": "40d6cf1d"
      },
      "source": [
        "## Соединения \n",
        "\n",
        "Чтобы собрать все воедино, полезно собрать и сравнить некоторые формулы, представленные выше. \n",
        "\n",
        "Сначала рассмотрим SVD матрицы $m\\times n$: \n",
        "\n",
        "$$ \n",
        "X = U\\Sigma V^\\top \n",
        "$$ \n",
        "\n",
        "Вычислить: \n",
        "\n",
        "\n",
        "<a id='equation-eq-xxcompare'></a> \n",
        "$$ \n",
        "\\begin{aligned} \n",
        "XX^\\top &=U\\Sigma V^\\top V\\Sigma^\\top U^\\top \\cr \n",
        "&\\equiv U\\Sigma\\Sigma^\\top U^\\top \\cr \n",
        "&\\equiv U\\Lambda U^\\top \n",
        "\\end{aligned} \\tag{5.12} \n",
        "$$ \n",
        "\n",
        "Сравните представление [(5.12)](#equation-eq-xxcompare) с уравнением [(5.11)](#equation-eq-xxo) выше. \n",
        "\n",
        "Очевидно, $U$ в СВД — это матрица $P$ \n",
        "Собственные векторы $XX^\\top$ и $\\Sigma\\Sigma^\\top$ — это матрица $\\Lambda$ собственных значений. \n",
        "\n",
        "Во-вторых, давайте посчитаем \n",
        "\n",
        "$$ \n",
        "\\begin{aligned} \n",
        "X^\\top X &=V\\Sigma^\\top U^\\top U\\Sigma V^\\top \\\\ \n",
        "&=V\\Sigma^\\top {\\Sigma}V^\\top \n",
        "\\end{aligned} \n",
        "$$ \n",
        "\n",
        "Таким образом, матрица $V$ в СВД является матрицей собственных векторов $X^\\top X$ \n",
        "\n",
        "Суммируя и сопоставляя все вместе, мы имеем собственное разложение выборки \n",
        "ковариационная матрица \n",
        "\n",
        "$$ \n",
        "X X^\\top = P \\Lambda P^\\top \n",
        "$$ \n",
        "\n",
        "где $P$ — ортогональная матрица. \n",
        "\n",
        "Далее, из СВД $X$ мы знаем, что \n",
        "\n",
        "$$ \n",
        "X X^\\top = U \\Sigma \\Sigma^\\top U^\\top \n",
        "$$ \n",
        "\n",
        "где $U$ — ортогональная матрица. \n",
        "\n",
        "Таким образом, $P = U$ и мы имеем представление $X$ \n",
        "\n",
        "$$ \n",
        "X = P \\epsilon = U \\Sigma V^\\top \n",
        "$$ \n",
        "\n",
        "Отсюда следует, что \n",
        "\n",
        "$$ \n",
        "U^\\top X = \\Sigma V^\\top = \\epsilon \n",
        "$$ \n",
        "\n",
        "Обратите внимание, что из предыдущего следует, что \n",
        "\n",
        "$$ \n",
        "\\epsilon \\epsilon^\\top = \\Sigma V^\\top V \\Sigma^\\top = \\Sigma \\Sigma^\\top = \\Lambda , \n",
        "$$ \n",
        "\n",
        "чтобы все сходилось. \n",
        "\n",
        "Ниже мы определяем класс DecomAnaанализ, который объединяет PCA и SVD для заданной матрицы данных X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d696f380",
      "metadata": {
        "hide-output": false,
        "id": "d696f380"
      },
      "outputs": [],
      "source": [
        "class DecomAnalysis:\n",
        "    \"\"\"\n",
        "    Класс для проведения PCA и SVD.\n",
        "    X: data matrix\n",
        "    r_component: chosen rank for best approximation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, r_component=None):\n",
        "         ваш_код\n",
        "        \n",
        "\n",
        "    def pca(self):\n",
        "         ваш_код\n",
        "\n",
        "    def svd(self):\n",
        "         ваш_код\n",
        "        \n",
        "\n",
        "    def fit(self, r_component):\n",
        "\n",
        "        # pca\n",
        "         ваш_код\n",
        "\n",
        "        # transform data\n",
        "         ваш_код\n",
        "\n",
        "        # svd\n",
        "         ваш_код\n",
        "\n",
        "        # transform data\n",
        "         ваш_код\n",
        "\n",
        "\n",
        "def diag_sign(A):\n",
        "    \"Compute the signs of the diagonal of matrix A\"\n",
        "\n",
        "    D =  ваш_код\n",
        "\n",
        "    return D"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "891464b2",
      "metadata": {
        "id": "891464b2"
      },
      "source": [
        "Мы также определяем функцию, которая выводит информацию, чтобы мы могли сравнивать разложения. \n",
        "полученные по разным алгоритмам."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1bc72aa1",
      "metadata": {
        "hide-output": false,
        "id": "1bc72aa1"
      },
      "outputs": [],
      "source": [
        "def compare_pca_svd(da):\n",
        "    \"\"\"\n",
        "    Compare the outcomes of PCA and SVD.\n",
        "    \"\"\"\n",
        "\n",
        "    da.pca()\n",
        "    da.svd()\n",
        "\n",
        "    print('Eigenvalues and Singular values\\n')\n",
        "    print(f'λ = {da.λ}\\n')\n",
        "    print(f'σ^2 = {da.σ**2}\\n')\n",
        "    print('\\n')\n",
        "\n",
        "    # loading matrices\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    plt.suptitle('loadings')\n",
        "    axs[0].plot(da.P.T)\n",
        "    axs[0].set_title('P')\n",
        "    axs[0].set_xlabel('m')\n",
        "    axs[1].plot(da.U.T)\n",
        "    axs[1].set_title('U')\n",
        "    axs[1].set_xlabel('m')\n",
        "    plt.show()\n",
        "\n",
        "    # principal components\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    plt.suptitle('principal components')\n",
        "    axs[0].plot(da.ε.T)\n",
        "    axs[0].set_title('ε')\n",
        "    axs[0].set_xlabel('n')\n",
        "    axs[1].plot(da.VT[:da.r, :].T * np.sqrt(da.λ))\n",
        "    axs[1].set_title(r'$V^\\top *\\sqrt{\\lambda}$')\n",
        "    axs[1].set_xlabel('n')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5fbc462",
      "metadata": {
        "id": "f5fbc462"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf483657",
      "metadata": {
        "id": "bf483657"
      },
      "source": [
        "## Упражнение\n",
        "\n",
        "В методе обычных наименьших квадратов (OLS) мы учимся вычислять $ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $, но бывают случаи, например, когда у нас есть коллинеарность или недоопределенная система: **короткая толстая** матрица. \n",
        "\n",
        "В этих случаях матрица $(X^\\top X)$ не является необратимой (ее определитель равен нулю) или плохо обусловленной (ее определитель очень близок к нулю). \n",
        "\n",
        "Вместо этого мы можем создать так называемую [псевдообратную](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), аппроксимацию инвертированной матрицы полного ранга, чтобы мы могли вычислить с ее помощью $ \\hat{\\beta} $. \n",
        "\n",
        "Думая в терминах теоремы Эккарта-Янга, постройте псевдообратную матрицу $ X^{+} $ и используйте ее для вычисления $ \\hat{\\beta} $."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "date": 1768362878.4850564,
    "filename": "svd_intro.md",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "title": "Singular Value Decomposition (SVD)"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
