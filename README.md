# Кластеризация и методы понижения разметроности

## Общая структура курса

- Уровень: магистратура, продвинутый курс по машинному обучению.
- Формат: 12 лекций (по теории и методам) + 12 практик (Python, sklearn, umap‑learn, реальные датасеты).
- Итоговая оценка: групповой или индивидуальный проект по применению методов понижения размерности и кластеризации к реальным данным.

***

## Лекции (12 тем)

1. **Введение в понижение размерности и кластеризацию**  
   - Задачи несупервизируемого обучения, мотивирующие примеры (визуализация высоких измерений, сжатие, шумоподавление, кластерный анализ).[2][1]
   - Обзор основных классов методов: линейные (PCA), manifold‑методы (Isomap, LLE, t‑SNE, UMAP), метрические и модельные методы кластеризации (k‑means, иерархические, плотностные, GMM).[6][7][8]

2. **Линейные методы: PCA и вариации**  
   - Математика PCA: ковариационная матрица, собственные значения/векторы, объясняемая дисперсия.[3][2]
   - Инкрементальный PCA, randomized PCA, SVD‑подход, связь с линейной регрессией и факторным анализом.[5][2]

3. **Kernel PCA и другие линейно‑нелинейные обобщения**  
   - Общая схема kernel‑методов, kernel PCA и выбор ядра.[9]
   - Сравнение с классическим PCA, плюсы и минусы, вычислительная сложность и регуляризация.[5][9]

4. **Манifold learning: MDS, Isomap, LLE, Laplacian Eigenmaps**  
   - Классическая MDS, геодезические расстояния и идея Isomap.[10][6]
   - LLE, Laplacian Eigenmaps: графы соседства, локальная структура, спектральное разложение.[11][7]

5. **t‑SNE: вероятностная постановка и оптимизация**  
   - Стохастическое соседство, распределения в высоком и низком пространстве, Kullback–Leibler‑дивергенция.[7][11]
   - Особенности: perplexity, crowding problem, интерпретация проекций и типичные ошибки использования.[7][5]

6. **UMAP и современные DR‑методы (TriMAP, PaCMAP, PHATE)**  
   - Теоретические идеи UMAP: топологический подход, графы и fuzzy‑множества.[5][7]
   - Обзор TriMAP, PaCMAP, PHATE и сравнение с t‑SNE по качеству, скорости и устойчивости к препроцессингу.[7][5]

7. **Основы кластеризации: постановка задач и метрики**  
   - Понятие кластеров, внутрикластерная/межкластерная вариабельность, расстояния и меры сходства.[12][8]
   - Внешние и внутренние метрики качества: silhouette score, Calinski–Harabasz, Davies–Bouldin, кратко об индексах на графах.[13][12]

8. **Разделяющие методы: k‑means и его варианты**  
   - Алгоритм k‑means, сходимость, выбор k (elbow, silhouette), чувствительность к инициализации.[14][15]
   - K‑means++, mini‑batch k‑means, k‑modes/k‑prototypes для категориальных и смешанных данных.[15][13]

9. **Иерархическая кластеризация**  
   - Агломеративные и дивизивные стратегии, linkage (single/complete/average/Ward).[13][12]
   - Дендограммы, выбор числа кластеров, вычислительная сложность и практические рекомендации.[16][12]

10. **Плотностные методы: DBSCAN, OPTICS и HDBSCAN**  
    - Основные идеи DBSCAN: eps, minPts, core/ border/ noise, устойчивость к выбросам.[17][12]
    - OPTICS и HDBSCAN: иерархическое расширение DBSCAN, преимущества на сложных формах кластеров.[8][17]

11. **Модель‑основанные методы: GMM и EM‑алгоритм**  
    - Gaussian Mixture Models, предположения о распределении, soft‑кластеризация.[15][8]
    - EM‑алгоритм, выбор числа компонент (BIC/AIC), связь с k‑means.[8][15]

12. **Интеграция DR и кластеризации, устойчивость и интерпретация**  
    - Пайплайны: стандартизация → DR (PCA/UMAP) → кластеризация (k‑means/DBSCAN/иер.) и влияние каждого шага.[12][2]
    - Оценка устойчивости кластеров к инициализации, шуму и параметрам DR; краткий обзор приложений (биоинформатика, изображение, тексты).[5][7]

***

## Практические занятия (12 работ)

1. **Python‑экосистема для DR и кластеризации**  
   - Обзор библиотек: numpy, pandas, scikit‑learn, umap‑learn; загрузка и первичный анализ датасета (Iris, MNIST subset и др.).[3][2]
   - Реализация базовой визуализации высокомерных данных (pairplots, heatmaps) и предварительная нормализация.[12][3]

2. **PCA на реальных данных**  
   - Реализация PCA с нуля (через SVD) и с помощью sklearn; анализ объясняемой дисперсии.[2][3]
   - Визуализация в 2D/3D, интерпретация главных компонент и влияние стандартизации.[2][5]

3. **Kernel PCA и сравнение с линейной PCA**  
   - Применение kernel PCA (RBF, poly kernels) к синтетическим нелинейным датасетам (круги, спирали).[9][10]
   - Сравнение качества визуализации и сохранения структуры с линейной PCA.[9][5]

4. **Isomap, LLE и спектральные методы**  
   - Применение Isomap и LLE к классическим manifold‑датасетам (Swiss roll и др.).[6][10]
   - Исследование влияния числа соседей и размерности на качество развёртки многообразия.[11][7]

5. **Практика с t‑SNE**  
   - Использование t‑SNE для визуализации рукописных цифр или эмбеддингов текстов; подбор perplexity, learning rate.[3][7]
   - Сравнение результатов t‑SNE и PCA по качеству разделения и устойчивости к параметрам.[7][5]

6. **Практика с UMAP и современными DR‑алгоритмами**  
   - Применение UMAP и сравнение с t‑SNE по скорости и структуре кластеров.[3][7]
   - Изучение чувствительности к препроцессингу (разные нормировки, отбор признаков) на одном датасете.[5][7]

7. **Метрики кластеризации и baseline‑эксперименты**  
   - Реализация и использование silhouette score, Calinski–Harabasz, Davies–Bouldin для разных разбиений данных.[13][12]
   - Сравнение разных алгоритмов на одном DR‑представлении по внутренним и, при наличии разметки, внешним метрикам.[4][12]

8. **k‑means и его модификации**  
   - Реализация k‑means (в т.ч. k‑means++) и его применение к данным после PCA/UMAP.[14][15]
   - Эксперименты с выбором k и инициализацией, анализ устойчивости решения.[15][12]

9. **Иерархическая кластеризация и дендограммы**  
   - Реализация агломеративной кластеризации с различными linkage на данных после PCA.[13][12]
   - Построение и анализ дендограмм, выбор уровня отсечения и числа кластеров.[16][12]

10. **DBSCAN, OPTICS, HDBSCAN**  
    - Применение DBSCAN/OPTICS к нелинейно разделимым данным; подбор eps, minPts и других параметров.[17][8]
    - Сравнение с k‑means и иерархической кластеризацией на тех же DR‑представлениях.[17][12]

11. **GMM и мягкая кластеризация**  
    - Обучение GMM на данных, уменьшенных с помощью PCA или UMAP; визуализация вероятностных принадлежностей.[8][15]
    - Сравнение GMM с k‑means/DBSCAN по метрикам качества и интерпретируемости.[12][8]

12. **Мини‑проект: от данных к интерпретации**  
    - Студенты выбирают небольшой датасет, строят полный пайплайн: препроцессинг → DR → кластеризация → оценка качества.[4][2]
    - Подготовка краткого отчёта (ноутбук + слайды) как репетиция финального проекта.[4][3]

***

## Тема финального проекта

**Тема:**  
«Исследование и сравнение методов понижения размерности и кластеризации на реальном высокоразмерном датасете (из области по выбору: биоинформатика, тексты, изображения, графы) с анализом устойчивости и интерпретируемости полученных кластеров».[7][5]

**Основные требования к проекту:**  
- Выбор и обоснование одного или двух реальных высокоразмерных датасетов (например, single‑cell RNA‑seq, embedding‑представления текстов, feature‑описания изображений).[2][5]
- Построение и сравнение нескольких пайплайнов «DR → кластеризация» (минимум 3 DR‑метода и 3 алгоритма кластеризации) с оценкой качества, устойчивости и практической интерпретации кластеров.[4][12][7]

[1](http://www.stat.yale.edu/~lc436/Stat675-Syllabus.pdf)
[2](https://mlcourse.ai/book/topic07/topic7_pca_clustering.html)
[3](https://www.datacamp.com/courses/dimensionality-reduction-in-python)
[4](https://www.coursera.org/learn/clustering-analysis)
[5](https://pmc.ncbi.nlm.nih.gov/articles/PMC9296444/)
[6](https://www.kaggle.com/questions-and-answers/59551)
[7](https://jmlr.org/papers/volume22/20-1061/20-1061.pdf)
[8](https://python.plainenglish.io/clustering-in-machine-learning-k-means-hierarchical-dbscan-optics-gmm-explained-bd01e9484dac)
[9](https://encord.com/blog/dimentionality-reduction-techniques-machine-learning/)
[10](https://fiveable.me/lists/dimensionality-reduction-algorithms)
[11](https://math.gmu.edu/~berry/Publications/DraganovThesis.pdf)
[12](https://hex.tech/blog/comparing-density-based-methods/)
[13](https://www.trainindata.com/p/clustering-and-dimensionality-reduction)
[14](https://towardsdatascience.com/k-means-dbscan-gmm-agglomerative-clustering-mastering-the-popular-models-in-a-segmentation-c891a3818e29/)
[15](https://github.com/Sarvandani/Machine_learning_9_algorithms_of_clustering)
[16](https://www.hse.ru/en/edu/courses/375270341)
[17](https://www.niser.ac.in/~smishra/teach/cs460/23cs460/lectures/lec16.pdf)
[18](https://learningforlife.tudelft.nl/ai-skills-introduction-to-unsupervised/)
[19](https://raw.githubusercontent.com/anthonyozerov/cdr-cookbook/main/cdr-cookbook.pdf)
[20](https://www.youtube.com/watch?v=T8MkpOAow9o)
